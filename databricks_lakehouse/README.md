# Databricks Lakehouse - Real-Time Customer Behavior Analytics

A complete end-to-end Databricks Lakehouse implementation featuring real-time streaming, medallion architecture, AI/ML inference, and comprehensive governance.

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ ARCHITECTURE.md              # Architecture documentation
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ workspace_config.json    # Databricks workspace settings
â”‚   â”œâ”€â”€ kafka_config.json        # Kafka connection settings
â”‚   â””â”€â”€ unity_catalog_config.sql # Unity Catalog setup
â”œâ”€â”€ infrastructure/              # Infrastructure as Code
â”‚   â”œâ”€â”€ terraform/               # Terraform configurations
â”‚   â””â”€â”€ databricks_cluster.json  # Cluster configuration
â”œâ”€â”€ notebooks/                   # Databricks notebooks
â”‚   â”œâ”€â”€ 01_bronze_ingestion.py   # Streaming ingestion
â”‚   â”œâ”€â”€ 02_silver_transformation.py # Data cleaning & enrichment
â”‚   â”œâ”€â”€ 03_gold_aggregation.py   # Analytics aggregations
â”‚   â”œâ”€â”€ 04_ml_training.py        # ML model training
â”‚   â”œâ”€â”€ 05_ml_inference.py       # Real-time inference
â”‚   â”œâ”€â”€ 06_governance_setup.py   # Unity Catalog setup
â”‚   â”œâ”€â”€ 07_monitoring.py         # Monitoring & alerting
â”‚   â”œâ”€â”€ 08_bi_queries.py         # BI queries & dashboards
â”‚   â””â”€â”€ 09_performance_optimization.py # Performance optimization
â”œâ”€â”€ workflows/                   # Orchestration
â”‚   â”œâ”€â”€ dlt_pipeline.py          # Delta Live Tables pipeline
â”‚   â”œâ”€â”€ workflow_definition.json # Workflow configuration
â”‚   â””â”€â”€ optimization_schedule.json # Performance optimization schedule
â”œâ”€â”€ tests/                       # Test cases
â”‚   â”œâ”€â”€ test_bronze.py
â”‚   â”œâ”€â”€ test_silver.py
â”‚   â””â”€â”€ test_gold.py
â””â”€â”€ utils/                       # Utility functions
    â”œâ”€â”€ data_quality.py          # Data quality checks
    â”œâ”€â”€ monitoring_utils.py      # Monitoring helpers
    â””â”€â”€ performance_optimizer.py # Performance optimization utilities
```

## ğŸ“ˆ Performance Optimization

See [PERFORMANCE_OPTIMIZATION.md](PERFORMANCE_OPTIMIZATION.md) for comprehensive performance tuning guide.

**Key Optimizations:**
- âœ… Delta Lake auto-optimize enabled
- âœ… Z-ordering for multi-column queries
- âœ… Bloom filters for fast lookups
- âœ… Adaptive Query Execution (AQE)
- âœ… Delta cache for frequently accessed data
- âœ… Automated optimization schedules

## ğŸš€ Quick Start

### Prerequisites

- Databricks workspace with Unity Catalog enabled
- Access to Kafka/Event Hubs/Kinesis
- Databricks CLI configured
- Python 3.8+ (for local development)

### Setup Steps

1. **Configure Unity Catalog**
   ```bash
   databricks workspace import config/unity_catalog_config.sql
   ```

2. **Deploy Infrastructure**
   ```bash
   cd infrastructure/terraform
   terraform init
   terraform plan
   terraform apply
   ```

3. **Import Notebooks**
   ```bash
   databricks workspace import_dir notebooks/ /Shared/lakehouse/
   ```

4. **Configure Streaming Source**
   - Update `config/kafka_config.json` with your Kafka broker details
   - Set up authentication credentials in Databricks secrets

5. **Run Initial Setup**
   - Execute `06_governance_setup.py` to create catalogs and schemas
   - Run `04_ml_training.py` to train the initial model

6. **Start Streaming Pipeline**
   - Deploy `workflows/dlt_pipeline.py` as a Delta Live Table
   - Or schedule notebooks as Databricks Jobs

## ğŸ“Š Use Case: Customer Behavior Analytics

### Data Schema

**Bronze (Raw Events)**
- `event_id`: Unique event identifier
- `customer_id`: Customer identifier
- `event_type`: click, purchase, review, page_view
- `timestamp`: Event timestamp
- `raw_data`: JSON payload with event details

**Silver (Cleaned)**
- All Bronze fields + enriched data
- `product_id`: Product identifier (enriched)
- `category`: Product category
- `customer_segment`: Customer segment
- `text_content`: Review text (for sentiment)
- `is_valid`: Data quality flag

**Gold (Analytics)**
- `customer_id`, `date`
- `total_events`, `total_purchases`, `total_revenue`
- `avg_sentiment_score`: Average sentiment from reviews
- `behavior_features`: ML-ready feature vector

### ML Model: Sentiment Classification

- **Input**: Review text from customer events
- **Output**: Sentiment score (positive/negative/neutral)
- **Training Data**: Gold aggregated customer reviews
- **Inference**: Real-time scoring via `ai_query` or Model Serving

## ğŸ” Governance

All tables are registered in Unity Catalog with:
- **Catalogs**: `lakehouse` (production), `lakehouse_dev` (development)
- **Schemas**: `bronze`, `silver`, `gold`
- **Access Control**: Role-based permissions
- **Lineage**: Automatic tracking via Unity Catalog

## ğŸ“ˆ Monitoring

Key metrics tracked:
- Streaming lag (Bronze ingestion)
- Data quality scores (Silver validation)
- ML model performance (accuracy, latency)
- Processing throughput
- Error rates and failures

## ğŸ“ˆ Performance Optimization

See [PERFORMANCE_OPTIMIZATION.md](PERFORMANCE_OPTIMIZATION.md) for comprehensive performance tuning guide.

**Key Optimizations:**
- âœ… Delta Lake auto-optimize enabled
- âœ… Z-ordering for multi-column queries
- âœ… Bloom filters for fast lookups
- âœ… Adaptive Query Execution (AQE)
- âœ… Delta cache for frequently accessed data
- âœ… Automated optimization schedules

## ğŸ§ª Testing

Run test suite:
```bash
pytest tests/ -v
```

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## ğŸ“§ Support

For issues and questions, please open a GitHub issue.
